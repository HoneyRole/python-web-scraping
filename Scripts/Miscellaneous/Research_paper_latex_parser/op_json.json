[
    {
        "author": "I.M. Great and So.R. Yu",
        "title": "A Sample Research Paper",
        "Introduction": " \n\nUsing latex is pretty easy if you have a sample document you can follow.Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed volutpat ornare odio et faucibus. Donec fringilla massa eget auctor viverra. Mauris a imperdiet est. Cras tincidunt nulla ut elit tristique ultricies. Phasellus nec orci vel mi suscipit maximus at vitae tortor. Vivamus sed libero vel lacus aliquam rhoncus. Ut in lacinia nunc. Nullam quis mauris leo. Phasellus vitae nisl condimentum quam congue volutpat. Quisque et dapibus ipsum. Curabitur fringilla pellentesque elit, non posuere purus malesuada id. Pellentesque rutrum vitae urna eu mattis.\n\nMaecenas ac congue massa. Quisque a sem turpis. Duis et diam ex. Suspendisse et enim interdum, sodales risus eu, ultrices est. Suspendisse eu odio enim. In vulputate odio porttitor tincidunt vestibulum. Praesent tincidunt ullamcorper purus, quis semper felis volutpat quis.\n\n",
        "Results": " \nIncluding figures, tables, and equations is easy. Latex also permits easy reference to document elements (figures, tables, sections). Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tincidunt lorem luctus eros dictum faucibus. Fusce euismod libero et erat pretium dapibus. Pellentesque faucibus hendrerit est, ac fringilla urna. In porta, ante eu dictum vestibulum, nisl nulla euismod purus, ac bibendum nibh ante vel elit. Fusce diam ante, tincidunt id eleifend a, hendrerit vitae tellus. Duis pretium urna ac vestibulum eleifend. Suspendisse potenti. Aliquam varius odio in pretium semper. Ut faucibus lobortis mauris vel sollicitudin. Nullam condimentum, lacus quis mattis pellentesque, massa nulla cursus nisi, aliquet eleifend est tellus ut libero.\n\n \n\n \n\n \n\n",
        "Conclusions": " \n\nMan, latex is great! Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tincidunt lorem luctus eros dictum faucibus. Fusce euismod libero et erat pretium dapibus. Pellentesque faucibus hendrerit est, ac fringilla urna. In porta, ante eu dictum vestibulum, nisl nulla euismod purus, ac bibendum nibh ante vel elit. Fusce diam ante, tincidunt id eleifend a, hendrerit vitae tellus. Duis pretium urna ac vestibulum eleifend. Suspendisse potenti. Aliquam varius odio in pretium semper. Ut faucibus lobortis mauris vel sollicitudin. Nullam condimentum, lacus quis mattis pellentesque, massa nulla cursus nisi, aliquet eleifend est tellus ut libero.\n\n",
        "Some_title": " \n\nTest title for user defined  section.\n\n",
        "user_defined_title_for_begin": " \n\nwjlrhfwer ljqr flwuer j rlferfurl u airlf  aiurf uoiruf iuoqir oiuqr iuq woe\n",
        "acknowledgement": "\nThe author is grateful to Donald Knuth for inventing tex, and making publication quality typesetting a reality for scientists around the world.\n\n"
    },
    {
        "author": "Pratik Merchant, Smit Moradiya, Jignesh Nagda, Niket Mehta",
        "title": "Parallel Implementation of Support Vector Machine",
        "Introduction": " \n\nThe name \u00e2\u0080\u0098support vectors\u00e2\u0080\u0099 (data points) used to define this dividing plane. Since we only require the SVs to create a classifier, the non SVs can be discarded. However, it becomes a problem when the points are not separable by a simple linear plane. Hence, to handle this problem, SVM uses what is known as the \u00e2\u0080\u009ckernel trick\u00e2\u0080\u009d on the training data and the mapping to a higher dimensional space is done by it, where such a dividing plane can be found more easily. Every improve accuracy. The role of Kernel is to transform the problem using some linear algebra for linear SVM and this is how the learning of the hyperplane happens. To avoid misclassifying each training example is given by the regularisation parameter. Lower is the regularisation value, more is the misclassification. To decide how far the influence of each training parameter reaches, the gamma parameter is used. Low gamma value means points which are at a far distance from the separation line are considered for calculation whereas a high gamma value implies that only the points nearby to the separation line are considered for calculation. Lastly, the separation of the line/hyperplane to the point which closest to it is called as margin. A larger separation for both the classes means a good margin and also no crossing into other class.\nThe steps to implement SVM are as follows: Step 1: Import all the necessary libraries such as numpy, pandas, matplotlib. Step 2: Importing the dataset Step 3: Performing exploratory data analysis Step 4: Performing data pre-processing Step 5: Splitting the data into train and test. Step 6: Import SVM, create a classifier and train the model. Step 6: Making predictions Step 7: Evaluating the algorithm  Step 8: Results \n\n",
        "my_section": " \n\nnlwekndw lweuidwe nulei nameiude includewe oiu wede oiuwe dn eiuqwend\n\n",
        "Results": " \nPSVM first loads the training data onto \u00e2\u0080\u0098m\u00e2\u0080\u0099 number of machines. This done in round robin fashion. The memory required by each memory is Big-Oh of nd/m. In the next step, PSVM performs a row-based ICF parallely on the data which has been loaded. At the end of this step, only a small portion of the factorized matrix is stored on each machine, which has a space complexity of Big-Oh of np/m. For the quadratic optimization problem, IPM is performed hereafter. Let, n- no. of training instances d-initial no. of dimensions p- After factorization, reduced matrix dimension (p<<<n) m- number of machines With the help of PSVM, the memory requirements reduce from a complexity of Big-Oh of n^2 to Big-Oh of np/m.  \n \nHDF5 which is a file format, data model, and a set of software allows users to store data and associated metadata. \n\nSeveral of its features make it useful for high performance computing and big data applications, such as data compression, which along with binary value storage can greatly reduce file sizes. It also allows parallel file access, which enables one or more processors to read/write data from a single file, and features customizable data \u00e2\u0080\u009cchunking\u00e2\u0080\u009d, which allows users to define how the data is internally arranged into subsets. This can be used to tune parallel performance. \n\nAn open source cluster computing system intended for the analysis and processing of big data is Apache Spark. Spark clusters have the ability to scale up to thousands of distributed nodes that can work cooperatively to process very large datasets in parallel, this makes them an example of an HTC system. It also supports real-time streaming processing of data as well as real-time streaming processing of data. Some of its main components are Spark streaming, Spark SQL, Spark core which are used for stream processing, to enable structured data processing and handle task distribution and scheduling respectively. (mllib) Machine Learning Library supports many tasks such as linear SVM training,linear regression and clustering. However some extra code needs to be written to handle multi-class problems since it only supports binary classification. \n \nThere maybe a period of CPU idle time if one segment finishes training before it concatenation counterpart. The issue can be cumulative in severely distributed databases. The benefits of attempting to balance the segments after the processing has started are likely to be outweighed by the overhead in MPI communication. This difference should be mitigated by proper randomization of the input vector order. \n \nApplications SVMs have been found particularly useful in earth observation and satellite imagery data analysis. Some of its other applications include: 1. Detetction of faces 2. Categorization of text and as well as hypertext 3. Image classifier 4. Fields related to biology 5. Remote homo-logy detection 6. Recognition of hand-written characters 7. GPC 8. Geo and Environmental Sciences \n\n",
        "Conclusions": " \n\nMany algorithmic approaches are found to be more effective when kernels are not used or memory is not a constraint. Also, other approaches can be used to achieve good speedup by dividing a serial algorithm into subtasks. These subtasks are basically subsets of the training data. If no. of machines continue to expand and cross the data-size independent threshold, PSVM cannot achieve linear speedup in such cases. There are 2 types of overheads encountered while implementing parallel SVM- communication and synchronization overheads. During message passing, communication time is accounted for.  Computation, communication and synchronization together form the running time. To increase the accuracy, PSVM must select the correct no. of machines.  Hence, we can conclude by saying that when the parallelism of modern hardware is leveraged, massive speedups are possible a satisfactory performance is achieved\n\n",
        "acknowledgement": "\nI am grateful to my college professors for providing me this wonderful oppurtunity to present this research paper.\n\n"
    },
    {
        "author": "Pratik Merchant",
        "title": "Prediction of human behaviour with the aid of sentiment analysis using social media datasets.",
        "Introduction": " \n\nSocial media data like Facebook, Twitter, Instagram blogs, etc. is currently growing in an exploding speed. Sentiment analysis\u00e2\u0080\u0093also called opinion mining\u00e2\u0080\u0093is the process of defining and categorizing opinions in a given piece of text as positive, negative, or neutral. The main purpose of conducting this research is to understand the sentiments which in turn can help us mine knowledge and capture the ideas without necessarily going through all data, which will save us a huge amount of time. Also, this analysis can further be used for a variety of purposes such as identifying influencers, competitive benchmarking, consumer opinion and brand sentiment, etc.\nThe already existing models lack accuracy. Also, they predict on the basis of one or 2 factors which is too less a number considering the amount of thought processes a human brain goes through before coming on to a decision. Also, the inaccuracy occurring due to the automated bots need to be taken into consideration. Since, they can largely tilt the dataset to a particular side (positive or negative). \nThe main goal is to improve accuracy and also to remove the input of the bots from the datasets using appropriate filtering techniques. And also, to merge the prediction of all the various datasets together to obtain a cumulative prediction of all the social media accounts a person uses.\nThe Government or the common public can largely benefit from this since any negative event(protests) if predicted by the model may help in taking adequate protective measures and hence in turn maybe avoid or reduce the magnitude of the same. This issue if addressed before could have prevented the negative impacts of a lot of events such as the Muzaffarnagar riots, FTII Agitation, Pro-Jallikattu protests which took place in Tamil Nadu, etc. Hence, any such events if again predicted in the future, can very well be avoided by taking appropriate advance action.   \n\n",
        "Results": " \nThere are three machine learning classification algorithms that are predominantly used for sentiment analysis in social media and they are as follows:\na.\tSupport Vector Machines (SVMs)\nb.\tNaive-bayes\nc.\tDecision Trees\nEach has it\u00e2\u0080\u0099s own advantages and drawbacks; however, a few different studies have concluded that the Naive-Bayes classifier is the more accurate of the three.[1]\n      \nNaive-Bayes classifier is a machine learning classification algorithm that asserts an independent value for each feature within a dataset. In other words, each element is valued individually to determine a probability that the sum of these values will constitute a pre-defined label or outcome. Effective sentiment analysis of social media datasets using Naive Bayesian Classification involves extraction of subjective information from textual data. A normal human can easily understand the sentiment of a document written in natural language based on its knowledge of understanding the polarity of words and in some cases the general semantics used to describe the subject. The project aims to make the machine extract the polarity (positive, negative or neutral) of social media dataset with respect to the queried keyword.\nThis project introduces an approach for automatically classifying the sentiment of social media data by using the following procedure: First the training data is fed to the sentiment analysis engine for learning by using machine learning algorithm. The next step is to filter misleading data(mostly encountered because of bots).The next step involved is the training of the dataset by mathematical formulations. After the learning is complete with qualified accuracy, the machine starts accepting individual social data with respect to keyword that it analyses and interprets, and then classifies it as positive, negative or neutral with respect to the query term.[2] The prediction of an individual once obtained from the different social media datasets may then be cumulated and then compared with the prediction of other individuals to see if there is anything in common. Common predictions if found any may indicate the mass sentiment of the people and will also hint about their future course of actions if any.\nWhen talking about textual sentiment analysis, this usually comes in the form of a training set bag-of-words already sorted into positive or negative categories. A positive word may have a +1 scoring while a negative word will have a -1 scoring. You can also assign higher values to certain words that may be more negative in degree. Regardless, if the final score of a mention is positive, then the mention is positive and vice versa for negative final score.\nIf word only appears once, we don\u00e2\u0080\u0099t need a frequency table. If we assign each positive and negative value a \u00e2\u0080\u009c1\u00e2\u0080\u009d, then we can simply divide the positive and negative words by the amount of words in the entire mention and then the subtract the negative words score from the positive one and  if the total of our mention comes out as positive, we can say the sentiment of the mention above is positive and vice versa for a negative result.\nSince the total of our mention comes out as positive, we can say the sentiment of the mention above is positive. This is a pretty clear-cut case as we didn\u00e2\u0080\u0099t encounter polarizing words that might skew the result if a computer can\u00e2\u0080\u0099t understand which category the word belongs to.[1] \n\nNow, the maxim that more data will lead to better predictive models is not always true, because noise in the data can overwhelm predictive models. The ability to deal with noisy, incomplete, and inconsistent data will be at the heart of next-generation predictive models. For instance, when identifying \u00e2\u0080\u009cbots\u00e2\u0080\u009d on Twitter that are seeking to sway opinion to be positive about a political candidate, we needed to ignore the huge numbers of bots that were seeking to achieve other ends- such as spreading spam or seeking to influence opinions about other topics or to deceive users into clicking on links that generate revenue for the person who included that link in their tweet. Moreover, data about many Twitter handles are limited and, in some cases, intentionally misleading. Bot developers go to considerable effort to ensure that their bots elude detection.\n\nThe generation and reduction to practice of robust multistage predictive modeling for emergent phenomena is an important step. For instance, social movements have been classified into five stages: genesis of the movement, increase in social unrest, enthusiastic mobilization to develop an organization, maintenance of the organization, and termination (when the movement starts to die down). When the protest is in an early stage (for example, of people expressing grievances on Twitter), some stakeholders would benefit from a prediction of the likelihood of violence occurring in any of the future stages. In such extreme cases, identifying bots is a very important part.\nIn this way, the above proposed methodology if implemented, can be of great help in a variety of applications as seen above.\n\n\n",
        "Conclusions": " \n\nUltimately, once can say that sentiment analysis isn\u00e2\u0080\u0099t perfect, but neither are we when trying to decipher what someone means. Within social media monitoring, we need sentiment analysis as a starting point to understand general public sentiment in aggregate. \nHence, we can say that social media is perhaps the largest pool from which we can mine for public opinion and begin to gather informative data for prediction purposes. \nIn this way, I plan to complete the above mentioned process as soon as possible once begun. If done correctly, the process would be completed within a stipulated period of time. If I am successful in meeting my objectives then, this shall be largely benefical to the Government authorities, the Police authorities as well as the common people at large. \n\n",
        "acknowledgement": "\nI would like to thank my college professors for supporting me immensely in this endeavor.\n\n"
    },
    {
        "author": "\nAmeya Keskar                                                         Priya Mane\nChinmay Lotankar                                                     Jeet Mehta\n",
        "title": "C4.5 CLASSIFICATION ALGORITHM",
        "Introduction": " \n\nData mining is the process of analyzing large data and getting valuable information from it. There are various algorithms and techniques done to do so. One such data structure is Decision tree; it is a flowchart-like structure with nodes and arrows directing from one node to another. At each node, one attribute is considered and further split branches equal to the number of unique values the attribute can take. Each of this branch is connected to other node where the value of next attribute is defined. Hence in going from one node to another, we fix or determine the value of each attribute. Each leaf node consists of one of the values of classification variable.\nNow the question is which attribute must be placed at what level in a tree.C4.5 Algorithm is used to choose the attributes to be placed at each level of the tree. The main advantage of C4.5 algorithm can deal with attributes having numeric data/non-categorical data which is difficult to classify per say and also deal with missing value data.C4.5 algorithm makes a decision tree by using the concept of information entropy. The parameter used here is normalized information gain. Normalized information gain is calculated for each attribute and the one with maximum value is chosen as the first attribute/root node of the decision tree.\n\n\nliterature review\n\nFor the construction of a decision tree, we can use the C4.5 algorithm. The algorithm is based on Information gain entropy. We can say that, if an event is highly probable, there is no surprise if it occurs. This means that it gives very little information. This means amount of information gained is inversely proportional to the probability of the event. Entropy is proportional to the probability of an event; hence we can also say that information gain and entropy are inversely proportional.\nIn decision trees, it is necessary that with each split the entropy decreases. Hence, if the splitting is done accurately, we may arrive to a very definite decision. So, we check each node for all possible splitting. cases for First, we calculate the entropy difference and the case for which difference is least is considered..\n\nALGORITHM:\nCalculate Information gain for each parameter.\nDetermine the attribute with maximum Information gain entropy.\nChoose this attribute as next splitting node.\nContinue in similar manner for all attributes.\n \nLet us consider an example \n\n \n\n",
        "Results": " \nC4.5 Vs C5.0 C4.5 was superseded in 1997 by a commercial system See5/C5.0 (C5.0 for Unix / Linux, See5 pour Windows).  \nThe changes hold within new capabilities as well as much improved efficiency, and include: \n  A variant of boosting that constructs an ensemble of classifiers which are then voted to give a final classification. This often leads to a dramatic improvement in predictive accuracy.   New data types (e.g., dates), \u00e2\u0080\u009cnot applicable\u00e2\u0080\u009d values, variable misclassification costs, and mechanisms to pre-filter attributes. \n Unordered rule set when a case is classified, all applicable rules are found and voted. \n This improves both their predictive accuracy and the interpretability of rule sets.  \nMulti-threading enhances scalability. C5.0 have the ability to take advantage of computers with multiple CPUs and/or cores\n\n\n",
        "Conclusions": " \n\nThe decision tree is a usual algorithm in data mining.C4.5 algorithm is a wide application scope, high frequency decision tree algorithm. It constructs and prunes the decision tree analysis and estimates, completes the classified data mining by data preprocessing and choosing parameters or catalog.The article analyzes the C4.5 and improved methods for the calculation speed of C4.5 algorithm in detail. At least, it is proved by experiment data set that the improved C4.5 algorithm is well-performed on the training speed classify and accuracy. In this Paper C4.5 algorithm was improved the experiment proved that it has minimal impact on the classification accuracy, but the efficiency increased a lot. We can not only speed up the growing of the decision tree, so that better information of rules can be generated. In this paper the algorithm was verified by different large datasets which are publicly available on UCI\nmachine learning repository. With the improved algorithm ,we can get faster and more effective results without the change of the final decision and the presented algorithm constructs the decision tree more clear and understandable .Efficiency and classification is greatly improved and the disadvantages of low efficiency and memory consumption while dealing with large amount of data were overcome as it was in C4.5.If the amount of data is small original C4.5 is\nused because of its higher accuracy.\n\n\n",
        "thebibliography": " \n\nhttps://towardsdatascience.com/what-is-the-c4-5-algorithm-and-how-does-it-work-2b971a9e7db0\nhttps://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb\nhttps://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/\nhttps://arxiv.org/abs/1310.2071\nhttps://www.sciencedirect.com/science/article/pii/S0925231298000903\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/\n\n\n\n",
        "acknowledgement": "\nThe authors are grateful to K.J Somaiya college of Engineering faculty.\n\n"
    },
    {
        "author": "\nArman Cohan\nFranck Dernoncourt\nDoo Soon Kim\nTrung Bui\nSeokhwan Kim \nWalter Chang\nNazli Goharian\u00e2\u0080\u00a0\n",
        "title": "A Discourse-Aware Attention Model for\nAbstractive Summarization of Long Documents",
        "Introduction": " \nExisting large-scale summarization datasets\nconsist of relatively short documents. For exam\u0002ple, articles in the CNN/Daily Mail dataset (Her\u0002mann et al., 2015) are on average about 600 words\nlong. Similarly, existing neural summarization\nmodels have focused on summarizing sentences\nand short documents. In this work, we propose a\nmodel for effective abstractive summarization of\nlonger documents. Scientific papers are an ex\u0002ample of documents that are significantly longer\nthan news articles (see Table 1). They also fol\u0002low a standard discourse structure describing the\nproblem, methodology, experiments/results, and\nfinally conclusions (Suppe, 1998).\nMost summarization works in the literature\nfocus on extractive summarization. Examples\nof prominent approaches include frequency-based\nmethods (Vanderwende et al., 2007), graph-based\nmethods (Erkan and Radev, 2004), topic mod\u0002eling (Steinberger and Jezek, 2004), and neural\nmodels (Nallapati et al., 2017). Abstractive sum\u0002marization is an alternative approach where the\ngenerated summary may contain novel words and\nphrases and is more similar to how humans sum\u0002marize documents (Jing, 2002). Recently, neu\u0002ral methods have led to encouraging results in\nabstractive summarization (Nallapati et al., 2016;\nSee et al., 2017; Paulus et al., 2017; Li et al.,\n2017). These approaches employ a general frame\u0002work of sequence-to-sequence (seq2seq) models\n(Sutskever et al., 2014) where the document is\nfed to an encoder network and another (recurrent)\nnetwork learns to decode the summary. While\npromising, these methods focus on summarizing\nnews articles which are relatively short. Many\nother document types, however, are longer and\nstructured. Seq2seq models tend to struggle with\nlonger sequences because at each decoding step,\nthe decoder needs to learn to construct a context\nvector capturing relevant information from all the\ntokens in the source sequence (Shao et al., 2017).\nOur main contribution is an abstractive model\nfor summarizing scientific papers which are an\nexample of long-form structured document types.\nOur model includes a hierarchical encoder, captur\u0002ing the discourse structure of the document and a\ndiscourse-aware decoder that generates the sum\u0002mary. Our decoder attends to different discourse\nsections and allows the model to more accurately\nrepresent important information from the source\nresulting in a better context vector. We also in\u0002troduce two large-scale datasets of long and struc\u0002tured scientific papers obtained from arXiv and\nPubMed to support both training and evaluating\nmodels on the task of long document summariza\u0002tion. Evaluation results show that our method out\u0002performs state-of-the-art summarization models1.\n\n\n",
        "Results": " \nOur main results are shown in Tables 2\nand 3. Our model significantly outperforms the\nstate-of-the-art abstractive methods, showing its\neffectiveness on both datasets. We observe that\nin our ROUGE-1 score is respectively about 4 and\n3 points higher than the abstractive model PntrGen-Seq2Seq for the arXiv and PubMed datasets,\nproviding a significant improvement. Our method\nalso outperforms most of the extractive methods\nexcept for LexRank in one of the ROUGE scores.\nWe note that since extractive methods copy salient\nsentences from the document, it is usually easier for them to achieve higher ROUGE scores.\nFigure 2 illustrates the effectiveness of our\nmodel extensions in capturing various discourse\ninformation from the papers. It can be observed\nthat the state-of-the-art Pntr-Gen-Seq2Seq model\ngenerates a summary that mostly focuses on introducing the problem, whereas our model generates\na summary that includes more information about\nthe methodology and impacts of the target paper.\nThis indicates that the context vector in our model\ncompared with Pntr-Gen-Seq2Seq is better able to\ncapture important information from the source by\nattending to various discourse sections.\n\n\n",
        "Conclusions": " \nThis work was the first attempt at addressing\nneural abstractive summarization of single, long\ndocuments. We presented a neural sequence-tosequence model that is able to effectively summarize long and structured documents such as scientific papers. While our results are encouraging,\nthere is still much room for improvement for this\nchallenging task; our new datasets can help the\ncommunity to further explore this problem.\nWe note that following the convention in the\nsummarization research, our quantitative evaluation is performed by ROUGE automatic metric.\nWhile ROUGE is an effective evaluation framework, nuances in the coherence or coverage of the\nsummaries are not captured with it. It is non-trivial\nto evaluate such qualities especially for long doc\u0002ument summarization; future work can design expert human evaluations to explore these nuances.\n\n",
        "thebibliography": " \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 .\nSumit Chopra, Michael Auli, Alexander M Rush, and\nSEAS Harvard. 2016. Abstractive sentence summarization with attentive recurrent neural networks. In\nHLT-NAACL. pages 93\u00e2\u0080\u009398.\nArman Cohan and Nazli Goharian. 2015. Scientific article summarization using citation-context\nand article\u00e2\u0080\u0099s discourse structure. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 390\u00e2\u0080\u0093400. http://aclweb.org/\nanthology/D15-1045.\nArman Cohan and Nazli Goharian. 2017a. Contextu\u0002alizing citations for scientific summarization using\nword embeddings and domain knowledge. arXiv\npreprint arXiv:1705.08063 .\n\n",
        "acknowledgement": "\nWe thank the three anonymous reviewers for\ntheir comments and suggestions.\n\n"
    }
]